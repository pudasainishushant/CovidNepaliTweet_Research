{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d8adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08f3122",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"covid19_tweeter_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c822dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Label</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tokanize_tweet</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-01-10 22:06:41+00:00</td>\n",
       "      <td>अमेरिकामा कोभिड बाट एकै दिन चार हजारभन्दा बढीक...</td>\n",
       "      <td>अमेरिकामा,कोभिड,बाट,एकै,दिन,चार,हजारभन्दा,बढीक...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-01-10 17:49:34+00:00</td>\n",
       "      <td>कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक...</td>\n",
       "      <td>कोभिड,का,कारण,विदेशमा,रहेका,नेपालीहरुमा,मानसिक...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-10 16:18:34+00:00</td>\n",
       "      <td>नेपालमा क्लोभर बायोफार्मास्युटिकल्स अस्ट्रेलिय...</td>\n",
       "      <td>नेपालमा,क्लोभर,बायोफार्मास्युटिकल्स,अस्ट्रेलिय...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-10 15:12:17+00:00</td>\n",
       "      <td>कोभिड को खोप पनि लगाइयो</td>\n",
       "      <td>कोभिड,को,खोप,पनि,लगाइयो</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-01-10 15:07:12+00:00</td>\n",
       "      <td>अमेरिकामा कोभिड को नयाँ रेकर्ड एकै दिन हजारभन्...</td>\n",
       "      <td>अमेरिकामा,कोभिड,को,नयाँ,रेकर्ड,एकै,दिन,हजारभन्...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33469</th>\n",
       "      <td>33469</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-12 01:33:08+00:00</td>\n",
       "      <td>कोरोना भाइरसलाई विश्व स्वास्थ्य संगठनले दियो न...</td>\n",
       "      <td>कोरोना,भाइरसलाई,विश्व,स्वास्थ्य,संगठनले,नाम,कोभिड</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33470</th>\n",
       "      <td>33470</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-12 00:38:07+00:00</td>\n",
       "      <td>डब्ल्युएचओले दियाेको कोरोनाको नयाँ नाम कोभिड</td>\n",
       "      <td>डब्ल्युएचओले,दियाेको,कोरोनाको,नाम,कोभिड</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33471</th>\n",
       "      <td>33471</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-11 19:37:26+00:00</td>\n",
       "      <td>कोरोना भाइरस संक्रमणको औपचारिक नाम अब कोभिड</td>\n",
       "      <td>कोरोना,भाइरस,संक्रमणको,औपचारिक,नाम,कोभिड</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33472</th>\n",
       "      <td>33472</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-11 16:36:40+00:00</td>\n",
       "      <td>विश्व स्वास्थ्य संगठन डब्लुएचओ ले नोबल कोरोना ...</td>\n",
       "      <td>विश्व,स्वास्थ्य,संगठन,डब्लुएचओ,नोबल,कोरोना,भाइ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33473</th>\n",
       "      <td>33473</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-11 15:22:16+00:00</td>\n",
       "      <td>कोरोनाभाइरसका सङ्क्रमणको नाम कोभिड भनेर विश्व ...</td>\n",
       "      <td>कोरोनाभाइरसका,सङ्क्रमणको,नाम,कोभिड,विश्व,स्वास...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33474 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Label                   Datetime  \\\n",
       "0               0     -1  2021-01-10 22:06:41+00:00   \n",
       "1               1     -1  2021-01-10 17:49:34+00:00   \n",
       "2               2      1  2021-01-10 16:18:34+00:00   \n",
       "3               3      0  2021-01-10 15:12:17+00:00   \n",
       "4               4     -1  2021-01-10 15:07:12+00:00   \n",
       "...           ...    ...                        ...   \n",
       "33469       33469      0  2020-02-12 01:33:08+00:00   \n",
       "33470       33470      0  2020-02-12 00:38:07+00:00   \n",
       "33471       33471      0  2020-02-11 19:37:26+00:00   \n",
       "33472       33472      0  2020-02-11 16:36:40+00:00   \n",
       "33473       33473      0  2020-02-11 15:22:16+00:00   \n",
       "\n",
       "                                                   Tweet  \\\n",
       "0      अमेरिकामा कोभिड बाट एकै दिन चार हजारभन्दा बढीक...   \n",
       "1      कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक...   \n",
       "2      नेपालमा क्लोभर बायोफार्मास्युटिकल्स अस्ट्रेलिय...   \n",
       "3                                कोभिड को खोप पनि लगाइयो   \n",
       "4      अमेरिकामा कोभिड को नयाँ रेकर्ड एकै दिन हजारभन्...   \n",
       "...                                                  ...   \n",
       "33469  कोरोना भाइरसलाई विश्व स्वास्थ्य संगठनले दियो न...   \n",
       "33470       डब्ल्युएचओले दियाेको कोरोनाको नयाँ नाम कोभिड   \n",
       "33471        कोरोना भाइरस संक्रमणको औपचारिक नाम अब कोभिड   \n",
       "33472  विश्व स्वास्थ्य संगठन डब्लुएचओ ले नोबल कोरोना ...   \n",
       "33473  कोरोनाभाइरसका सङ्क्रमणको नाम कोभिड भनेर विश्व ...   \n",
       "\n",
       "                                          Tokanize_tweet Unnamed: 5  \n",
       "0      अमेरिकामा,कोभिड,बाट,एकै,दिन,चार,हजारभन्दा,बढीक...        NaN  \n",
       "1      कोभिड,का,कारण,विदेशमा,रहेका,नेपालीहरुमा,मानसिक...        NaN  \n",
       "2      नेपालमा,क्लोभर,बायोफार्मास्युटिकल्स,अस्ट्रेलिय...        NaN  \n",
       "3                                कोभिड,को,खोप,पनि,लगाइयो        NaN  \n",
       "4      अमेरिकामा,कोभिड,को,नयाँ,रेकर्ड,एकै,दिन,हजारभन्...        NaN  \n",
       "...                                                  ...        ...  \n",
       "33469  कोरोना,भाइरसलाई,विश्व,स्वास्थ्य,संगठनले,नाम,कोभिड        NaN  \n",
       "33470            डब्ल्युएचओले,दियाेको,कोरोनाको,नाम,कोभिड        NaN  \n",
       "33471           कोरोना,भाइरस,संक्रमणको,औपचारिक,नाम,कोभिड        NaN  \n",
       "33472  विश्व,स्वास्थ्य,संगठन,डब्लुएचओ,नोबल,कोरोना,भाइ...        NaN  \n",
       "33473  कोरोनाभाइरसका,सङ्क्रमणको,नाम,कोभिड,विश्व,स्वास...        NaN  \n",
       "\n",
       "[33474 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2411ec71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'प्रधानमँत्रीज्यू चुनाव गर्ने पैसा हुने तर कोभीड बाट जनताको लागि निशूल्क खोप ल्याउन पैसा नहुने खोप ल्याउन किन ढिलाइ गरिँदैछ देश र जनताप्रति किन लापरबाही'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d04891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length'] = data['Tokanize_tweet'].apply(\n",
    "    lambda row: min(len(row.split(\",\")), len(row)) if isinstance(row, str) else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8922dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data['length'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed98d659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक स्वास्थ्य सम्बन्धि समस्या देखिएको र आत्महत्याका घटनाहरु बढेको डा सापकोटाले बताए'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Tweet\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "177b8fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Label</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tokanize_tweet</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-01-10 22:06:41+00:00</td>\n",
       "      <td>अमेरिकामा कोभिड बाट एकै दिन चार हजारभन्दा बढीक...</td>\n",
       "      <td>अमेरिकामा,कोभिड,बाट,एकै,दिन,चार,हजारभन्दा,बढीक...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-01-10 17:49:34+00:00</td>\n",
       "      <td>कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक...</td>\n",
       "      <td>कोभिड,का,कारण,विदेशमा,रहेका,नेपालीहरुमा,मानसिक...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-10 16:18:34+00:00</td>\n",
       "      <td>नेपालमा क्लोभर बायोफार्मास्युटिकल्स अस्ट्रेलिय...</td>\n",
       "      <td>नेपालमा,क्लोभर,बायोफार्मास्युटिकल्स,अस्ट्रेलिय...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-10 15:12:17+00:00</td>\n",
       "      <td>कोभिड को खोप पनि लगाइयो</td>\n",
       "      <td>कोभिड,को,खोप,पनि,लगाइयो</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-01-10 15:07:12+00:00</td>\n",
       "      <td>अमेरिकामा कोभिड को नयाँ रेकर्ड एकै दिन हजारभन्...</td>\n",
       "      <td>अमेरिकामा,कोभिड,को,नयाँ,रेकर्ड,एकै,दिन,हजारभन्...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Label                   Datetime  \\\n",
       "0           0     -1  2021-01-10 22:06:41+00:00   \n",
       "1           1     -1  2021-01-10 17:49:34+00:00   \n",
       "2           2      1  2021-01-10 16:18:34+00:00   \n",
       "3           3      0  2021-01-10 15:12:17+00:00   \n",
       "4           4     -1  2021-01-10 15:07:12+00:00   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  अमेरिकामा कोभिड बाट एकै दिन चार हजारभन्दा बढीक...   \n",
       "1  कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक...   \n",
       "2  नेपालमा क्लोभर बायोफार्मास्युटिकल्स अस्ट्रेलिय...   \n",
       "3                            कोभिड को खोप पनि लगाइयो   \n",
       "4  अमेरिकामा कोभिड को नयाँ रेकर्ड एकै दिन हजारभन्...   \n",
       "\n",
       "                                      Tokanize_tweet Unnamed: 5  length  \n",
       "0  अमेरिकामा,कोभिड,बाट,एकै,दिन,चार,हजारभन्दा,बढीक...        NaN     9.0  \n",
       "1  कोभिड,का,कारण,विदेशमा,रहेका,नेपालीहरुमा,मानसिक...        NaN    18.0  \n",
       "2  नेपालमा,क्लोभर,बायोफार्मास्युटिकल्स,अस्ट्रेलिय...        NaN    20.0  \n",
       "3                            कोभिड,को,खोप,पनि,लगाइयो        NaN     5.0  \n",
       "4  अमेरिकामा,कोभिड,को,नयाँ,रेकर्ड,एकै,दिन,हजारभन्...        NaN    12.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18969988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    14998\n",
       "-1    13606\n",
       " 0     4870\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ce0c43d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_159392/825446640.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, AutoTokenizer, AutoModelForMaskedLM\n",
    "from scipy.spatial.distance import cosine \n",
    "import tokenizers \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import snowballstemmer \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import snowballstemmer\n",
    "import numpy\n",
    "import os \n",
    "import re\n",
    "import json\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98daed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_path = os.getcwd()+'/bert_model/vocab_low_data.txt'\n",
    "# model_path = os.getcwd()+'/blabla/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c99f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e86375ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizers = BertTokenizer.from_pretrained(vocab_path)\n",
    "# model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path=model_path, return_dict = True, output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b519b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"Shushant/nepaliBERT\", output_hidden_states = True, return_dict = True, output_attentions = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc414c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = AutoTokenizer.from_pretrained(\"Shushant/nepaliBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd3f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(model, open('bert_embeddings','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac250ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(tokenizers, open('bert_tokenizers','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b1e9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('bert_embeddings','rb'))\n",
    "tokenizers= pickle.load(open('bert_tokenizers','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8b082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2963c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1871cd20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['क',\n",
       " 'मौ',\n",
       " '##ज',\n",
       " '##दा',\n",
       " 'लोक',\n",
       " '##तान',\n",
       " '##तर',\n",
       " '##िक',\n",
       " 'वय',\n",
       " '##वस',\n",
       " '##था',\n",
       " 'राज',\n",
       " '##य',\n",
       " 'पन',\n",
       " '##ः',\n",
       " '##सर',\n",
       " '##चना',\n",
       " '##सग',\n",
       " 'जोडिएका',\n",
       " 'हिजोका',\n",
       " 'सवाल',\n",
       " '##हर',\n",
       " '##लाई',\n",
       " 'यथा',\n",
       " '##स',\n",
       " '##थिति',\n",
       " '##मा',\n",
       " 'छोड',\n",
       " '##र',\n",
       " 'सबल',\n",
       " 'होला',\n",
       " '?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers.tokenize(\"के मौजुदा लोकतान्त्रिक व्यवस्था राज्य पुनःसंरचनासँग जोडिएका हिजोका सवालहरूलाई यथास्थितिमा छोडेर सबल होला?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ca9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'अनि तेस्रो चिन्ता मौसम परिवर्तनले हिमशिखरहरूमा परेका आघातसँगसँगै सिमानावारिपारि नदीले ल्याएका प्रकोपहरू कसरी सम्हाल्ने'\n",
    "# marked_text = \" [CLS] \"+text+\" [SEP] \"\n",
    "# tokenized_text = tokenizer.tokenize(marked_text)\n",
    "# indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# segments_ids = [1] * len(indexed_tokens)\n",
    "\n",
    "# tokens_tensors = torch.tensor([indexed_tokens])\n",
    "# segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88a853e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     outputs = model(tokens_tensors, segments_tensors)\n",
    "#     hidden_states = outputs.hidden_states\n",
    "# #     print(hidden_states[-1])\n",
    "#     token_embeddings = hidden_states[-1]\n",
    "    \n",
    "#     token_embeddings = torch.squeeze(token_embeddings, dim = 0)\n",
    "    \n",
    "#     list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "#     print(list_token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc8c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nepali_stemmer = snowballstemmer.NepaliStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06bc3947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = ['तर','दुधमा तर बसेन|','तिम्रो घर आउन मन लाग्छ तर अल्छि लाग्छ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd86297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bert_text_preparation(text, tokenizer ):\n",
    "#     \"\"\"Preparing input for BERT\"\"\"\n",
    "    \n",
    "#     marked_text = \" [CLS] \" + text + \" [SEP] \"\n",
    "#     tokenized_text = tokenizer.tokenize(marked_text)\n",
    "#     indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "#     segments_ids = [1] * len(indexed_tokens) \n",
    "    \n",
    "#     # Convert inputs to Pytorch tensors\n",
    "#     tokens_tensors = torch.tensor([indexed_tokens])\n",
    "#     segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "#     return tokenized_text, tokens_tensors, segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a70ff12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "#     # Gradient claculation id disabled \n",
    "#     # Model is in inference mode\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(tokens_tensor, segments_tensors)\n",
    "#         # removing the first hidden state\n",
    "#         # the first state is the input state \n",
    "#         hidden_states = outputs.hidden_states\n",
    "    \n",
    "#     # Getting embeddings from final Bert Layer\n",
    "#     tokens_embeddings = hidden_states[-1]\n",
    "#     # Collasping the tensor into 1-dimension \n",
    "#     tokens_embeddings = torch.squeeze(tokens_embeddings, dim = 0)\n",
    "#     # Converting torchtensors to lists \n",
    "#     list_token_embeddings = [token_embed.tolist() for token_embed in tokens_embeddings]\n",
    "    \n",
    "#     return list_token_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03d8c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_text, tokens_tensors, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "# target_word_embeddings = []\n",
    "\n",
    "# for text in texts:\n",
    "#     tokenized_text, tokens_tensors, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "#     list_token_embeddings = get_bert_embeddings(tokens_tensors, segments_tensors, model)\n",
    "#     ## list_token_embeddings has embeddings of the given words\n",
    "# #     word_index = tokenized_text.index('तर')\n",
    "#     word_embeddings = [list_token_embeddings[token] for token in tokenized_text]\n",
    "# #     word_embedding = list_token_embeddings[word_index]\n",
    "# #     print(word_embedding)\n",
    "# #     target_word_embeddings.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1afd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1caf3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d4db1a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# target_word_embeddings = []\n",
    "\n",
    "# for text in texts:\n",
    "#     tokenized_text, tokens_tensors, segments_tensors = bert_text_preparation(text, tokenizers)\n",
    "#     list_token_embeddings = get_bert_embeddings(tokens_tensors, segments_tensors, model)\n",
    "# #     print(len(list_token_embeddings))\n",
    "#     ## list_token_embeddings has embeddings of the given words\n",
    "#     word_index = tokenized_text.index('तर')\n",
    "#     word_embedding = list_token_embeddings[word_index]\n",
    "# #     print(word_embedding)\n",
    "#     target_word_embeddings.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c79f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_word_embeddings[0] == target_word_embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb28025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0578cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(target_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5144fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_distances = []\n",
    "# for text1, embed1 in zip(texts, target_word_embeddings):\n",
    "#     for text2, embed2 in zip(texts, target_word_embeddings):\n",
    "#         cos_dist = 1 - cosine(embed1,embed2)\n",
    "#         list_of_distances.append([text1, text2, cos_dist])\n",
    "\n",
    "\n",
    "# distances_df = pd.DataFrame(list_of_distances, columns = ['text1','text2','distance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08820a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, AutoTokenizer, AutoModelForMaskedLM\n",
    "from scipy.spatial.distance import cosine \n",
    "import tokenizers \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import snowballstemmer \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import snowballstemmer\n",
    "import numpy\n",
    "import os \n",
    "import re\n",
    "import json\n",
    "import pickle \n",
    "\n",
    "# vocab_path = os.getcwd()+'/bert_model/vocab_low_data.txt'\n",
    "# model_path = os.getcwd()+'/blabla/'\n",
    "\n",
    "# vocab_path\n",
    "\n",
    "# tokenizers = BertTokenizer.from_pretrained(vocab_path)\n",
    "# model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path=model_path, return_dict = True, output_hidden_states = True)\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"Shushant/nepaliBERT\", output_hidden_states = True, return_dict = True, output_attentions = True)\n",
    "\n",
    "tokenizers = AutoTokenizer.from_pretrained(\"Shushant/nepaliBERT\")\n",
    "\n",
    "# pickle.dump(model, open('bert_embeddings','wb'))\n",
    "\n",
    "# pickle.dump(tokenizers, open('bert_tokenizers','wb'))\n",
    "\n",
    "model = pickle.load(open('bert_embeddings','rb'))\n",
    "tokenizers= pickle.load(open('bert_tokenizers','rb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizers.tokenize(\"के मौजुदा लोकतान्त्रिक व्यवस्था राज्य पुनःसंरचनासँग जोडिएका हिजोका सवालहरूलाई यथास्थितिमा छोडेर सबल होला?\")\n",
    "\n",
    "# text = 'अनि तेस्रो चिन्ता मौसम परिवर्तनले हिमशिखरहरूमा परेका आघातसँगसँगै सिमानावारिपारि नदीले ल्याएका प्रकोपहरू कसरी सम्हाल्ने'\n",
    "# marked_text = \" [CLS] \"+text+\" [SEP] \"\n",
    "# tokenized_text = tokenizer.tokenize(marked_text)\n",
    "# indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# segments_ids = [1] * len(indexed_tokens)\n",
    "\n",
    "# tokens_tensors = torch.tensor([indexed_tokens])\n",
    "# segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(tokens_tensors, segments_tensors)\n",
    "#     hidden_states = outputs.hidden_states\n",
    "# #     print(hidden_states[-1])\n",
    "#     token_embeddings = hidden_states[-1]\n",
    "    \n",
    "#     token_embeddings = torch.squeeze(token_embeddings, dim = 0)\n",
    "    \n",
    "#     list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "#     print(list_token_embeddings)\n",
    "\n",
    "# nepali_stemmer = snowballstemmer.NepaliStemmer()\n",
    "\n",
    "# texts = ['तर','दुधमा तर बसेन|','तिम्रो घर आउन मन लाग्छ तर अल्छि लाग्छ']\n",
    "\n",
    "# def bert_text_preparation(text, tokenizer ):\n",
    "#     \"\"\"Preparing input for BERT\"\"\"\n",
    "    \n",
    "#     marked_text = \" [CLS] \" + text + \" [SEP] \"\n",
    "#     tokenized_text = tokenizer.tokenize(marked_text)\n",
    "#     indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "#     segments_ids = [1] * len(indexed_tokens) \n",
    "    \n",
    "#     # Convert inputs to Pytorch tensors\n",
    "#     tokens_tensors = torch.tensor([indexed_tokens])\n",
    "#     segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "#     return tokenized_text, tokens_tensors, segments_tensors\n",
    "\n",
    "# def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "#     # Gradient claculation id disabled \n",
    "#     # Model is in inference mode\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(tokens_tensor, segments_tensors)\n",
    "#         # removing the first hidden state\n",
    "#         # the first state is the input state \n",
    "#         hidden_states = outputs.hidden_states\n",
    "    \n",
    "#     # Getting embeddings from final Bert Layer\n",
    "#     tokens_embeddings = hidden_states[-1]\n",
    "#     # Collasping the tensor into 1-dimension \n",
    "#     tokens_embeddings = torch.squeeze(tokens_embeddings, dim = 0)\n",
    "#     # Converting torchtensors to lists \n",
    "#     list_token_embeddings = [token_embed.tolist() for token_embed in tokens_embeddings]\n",
    "    \n",
    "#     return list_token_embeddings \n",
    "\n",
    "# tokenized_text, tokens_tensors, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "# target_word_embeddings = []\n",
    "\n",
    "# for text in texts:\n",
    "#     tokenized_text, tokens_tensors, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "#     list_token_embeddings = get_bert_embeddings(tokens_tensors, segments_tensors, model)\n",
    "#     ## list_token_embeddings has embeddings of the given words\n",
    "# #     word_index = tokenized_text.index('तर')\n",
    "#     word_embeddings = [list_token_embeddings[token] for token in tokenized_text]\n",
    "# #     word_embedding = list_token_embeddings[word_index]\n",
    "# #     print(word_embedding)\n",
    "# #     target_word_embeddings.append(word_embedding)\n",
    "\n",
    "# tokenized_text\n",
    "\n",
    "# type(tokenized_text[0])\n",
    "\n",
    "# target_word_embeddings = []\n",
    "\n",
    "# for text in texts:\n",
    "#     tokenized_text, tokens_tensors, segments_tensors = bert_text_preparation(text, tokenizers)\n",
    "#     list_token_embeddings = get_bert_embeddings(tokens_tensors, segments_tensors, model)\n",
    "# #     print(len(list_token_embeddings))\n",
    "#     ## list_token_embeddings has embeddings of the given words\n",
    "#     word_index = tokenized_text.index('तर')\n",
    "#     word_embedding = list_token_embeddings[word_index]\n",
    "# #     print(word_embedding)\n",
    "#     target_word_embeddings.append(word_embedding)\n",
    "\n",
    "# target_word_embeddings[0] == target_word_embeddings[1]\n",
    "\n",
    "\n",
    "\n",
    "# len(target_word_embeddings)\n",
    "\n",
    "# list_of_distances = []\n",
    "# for text1, embed1 in zip(texts, target_word_embeddings):\n",
    "#     for text2, embed2 in zip(texts, target_word_embeddings):\n",
    "#         cos_dist = 1 - cosine(embed1,embed2)\n",
    "#         list_of_distances.append([text1, text2, cos_dist])\n",
    "\n",
    "\n",
    "# distances_df = pd.DataFrame(list_of_distances, columns = ['text1','text2','distance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0c133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35bfb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
